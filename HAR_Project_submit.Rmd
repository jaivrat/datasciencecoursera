---
title: "Human Activity Monitor"
author: "Jai Vrat Singh"
date: "25 October 2015"
output: html_document
---

Goal of project is to predict/classify the excercise done by invividuals into Classes A,B,C,D,E. 
Classes descriptions are:
* exactly according to the specification (Class A)
* throwing the elbows to the front (Class B), 
* lifting the dumbbell only halfway (Class C), 
* lowering the dumbbell only halfway (Class D) 
* throwing the hips to the front (Class E).

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz3pZcj75Ns

Training dataset has 19622 obs. of  160 variables

```{r}
rm(list=ls())
setwd("/Users/jaivrat/work/coursera/machineLearning_jh")
#Training data set
training <- read.csv(file = "pml-training.csv", na.strings=c("NA","#DIV/0!", ""), header = TRUE)
#Testing data set
testing <- read.csv(file = "pml-testing.csv", na.strings=c("NA","#DIV/0!", ""), header = TRUE)
#We remove missing predictors
#There are lots of NA's, missing values as seen in summary
isAnyMissing <- sapply(training, function (x) any(is.na(x) | x == ""))
#We drop all those columns which have NAs
dropped <- colnames(training)[isAnyMissing]
#shrink training columns
training <- training[, !isAnyMissing]
#Since we need data only from belt, arm,forearm,dumbbell - and rest of the predictors not required
predictors <- colnames(training)[grepl("belt|[^(fore)]arm|dumbbell|forearm", colnames(training))]
training <- training[,c("classe", predictors )]
training$classe <- as.factor(training$classe)
#We have training set ready now, We similarly narrow down test set
testing <- testing[,c( predictors )]  #no classe here. Why?
```

We split the training data(19622x53), into sub datasets of training(60%) and testing(40%) for **crossvalidation**

```{r}
library(caret)
set.seed(1234)
inTrain = createDataPartition(training$classe, p=0.6,list=FALSE)
trainingSub = training[inTrain,]
testingSub  = training[-inTrain,]
```

Center and scale the data:
```{r}
#Train(Sub)
preProc <- preProcess(trainingSub[, -1])#First is classe, #Center+scale training data
centeredScaled <- predict(preProc, trainingSub[, -1])
centeredScaled <- data.frame(classe = trainingSub[, 1], centeredScaled)

#Test(Sub)
centeredScaledTest <- predict(preProc, testingSub[, -1])
centeredScaledTest <- data.frame(classe = testingSub[, 1], centeredScaledTest)
```

We can drop columns with no variation, but there are none
```{r}
#Check values with almost constan values or with almost no variation
zeroVariance <- nearZeroVar(centeredScaled, saveMetrics=TRUE)
zeroVariance[zeroVariance$nzv == TRUE,] #None
```

For fast processing we set up clusters.
```{r}
library(parallel)
library(doParallel)
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)

#Control parameters of the train function
ctrl <- trainControl(classProbs=TRUE,
                     savePredictions=TRUE,
                     allowParallel=TRUE)

```


**First** prediction model using Random forest:

```{r}
modelFit <- train(classe ~ ., data=centeredScaled, method="rf")
predictedValues <- predict(modelFit, centeredScaled)
# Predict on Train Data
confusionMatrix(predictedValues, centeredScaled[, "classe"])
#predict on Test Data
confusionMatrix(predict(modelFit, centeredScaledTest), centeredScaledTest[, "classe"])
```
As exptected, the Train(sub) data has high accuracy of 1 and there are no errors. Even, the test data shows a considerably good accuracy of 99.16% with 99% confidence in (98.93%, 99.35%)

Some most important variables in the Model Selection
```{r}
varImp(modelFit)
modelFit$finalModel
```

**Second** prediction model using decision tree.
```{r}
modelFitDT <- train(classe ~ ., data=centeredScaled, method="rpart")
# Predict on Train Data
predictionDT <- predict(modelFitDT, centeredScaled)
# Predict on Train Data
confusionMatrix(predictionDT, centeredScaled[, "classe"])
#predict on Test Data
confusionMatrix(predict(modelFitDT, centeredScaledTest), centeredScaledTest[, "classe"])
```

Second model has accuracy 49.89% with 99% confidence interval of (48.98%, 50.8%). Similary, the Test data shows a low accuracy of lower accuracy of 49.04% with 99% confidence interval of (47.93%, 50.16%).

**Conclusion**

We accept the First model using **Random forests** against the second(decision tree model) because of accuracy levels descibed above. Also, the 99% confidence interval is narrow, which means withhigh confidence we can use **Random forests** for prediction.

```{r}
#Prediction on test data(20 cases)
centeredScaled_newData <- predict(preProc, testing[,predictors])
# Predict on Train Data
prediction_newData <- predict(modelFit, centeredScaled_newData)
```

Write files for submission
```{r, echo = FALSE}
prediction_newData

# Write files for submission
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(prediction_newData)

stopCluster(cl)
```

